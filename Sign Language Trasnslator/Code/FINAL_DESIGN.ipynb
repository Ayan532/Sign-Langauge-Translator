{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29bc5aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------- IMPORTING PACKAGES------------------------------------------------\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "# from tensorflow.keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import layers, models\n",
    "from keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tkinter import *\n",
    "import cv2\n",
    "from PIL import ImageTk, Image\n",
    "\n",
    "import mediapipe as mp\n",
    "\n",
    "from tkinter import *\n",
    "from PIL import ImageTk, Image\n",
    "from gtts import gTTS \n",
    "from playsound import playsound\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "160b21b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is a Python class named Model that contains methods for building, saving, loading, and using a convolutional neural network (CNN) classifier for image classification. Here is a brief explanation of each method:\n",
    "\n",
    "# __init__(self, Type) is the constructor that initializes the classifier attribute of the class with a specific type of CNN model specified by the Type argument.\n",
    "\n",
    "# build_model(classifier) is a method that takes a CNN model as an input argument and adds layers to it. The layers consist of convolutional layers with ReLU activation, \n",
    "# batch normalization layers, max pooling layers, a flatten layer, and fully connected layers with ReLU activation and softmax activation for the output. The purpose of this method is to build the CNN model architecture.\n",
    "\n",
    "# save_classifier(path, classifier) is a method that saves the trained CNN model to a file specified by the path argument.\n",
    "\n",
    "# load_classifier(path) is a method that loads a trained CNN model from a file specified by the path argument.\n",
    "\n",
    "# predict(classes, classifier, img) is a method that takes a list of class names (classes), a trained CNN model (classifier), \n",
    "# and an image (img) as input arguments. It resizes the image to 64x64 pixels, converts it to a NumPy array, normalizes the pixel \n",
    "# values to the range [0,1], performs a forward pass through the CNN model, and returns the predicted class label (as a string) and \n",
    "# the predicted class probabilities (as a NumPy array).\n",
    "\n",
    "class Model:\n",
    "\n",
    "  classifier = None\n",
    "  def __init__(self, Type):\n",
    "    self.classifier = Type\n",
    "    \n",
    "  \n",
    "  def build_model(classifier):\n",
    "\n",
    "    classifier.add(Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (64,64,1)))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "    classifier.add(Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "    classifier.add(Dropout(0.2))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "    classifier.add(Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "    classifier.add(Flatten())\n",
    "    classifier.add(Dense(units = 512 , activation = 'relu'))\n",
    "    classifier.add(Dropout(0.3))\n",
    "    classifier.add(Dense(units = 29 , activation = 'softmax'))\n",
    "   \n",
    "\n",
    "\n",
    "    return classifier\n",
    "\n",
    "  def save_classifier(path, classifier):\n",
    "    classifier.save(path)\n",
    "\n",
    "  def load_classifier(path):\n",
    "    classifier = load_model(path)\n",
    "    return classifier\n",
    "\n",
    "\n",
    "\n",
    "  def predict(classes, classifier, img):\n",
    "    img = cv2.resize(img, (64, 64))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = img/255.0\n",
    "\n",
    "    pred = classifier.predict(img)\n",
    "    return classes[np.argmax(pred)], pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a5f84b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a Python class named DataGatherer that contains a method for loading and preprocessing images for image classification.\n",
    "# Here is a brief explanation of the method:\n",
    "\n",
    "# __init__(self, *args) is the constructor that initializes the dir attribute of the class with a directory path specified by the args\n",
    "# argument. If no argument is passed, it initializes the dir attribute with an empty string.\n",
    "\n",
    "# load_images(self) is a method that loads the images from the directory specified by the dir attribute, applies edge detection and resizing preprocessing functions on the images, and splits them into training and \n",
    "# testing datasets. It initializes empty lists for images and labels, loops over the folders in the dir directory and loads the images\n",
    "# from each folder into the images list, and assigns a numerical label to each image based on its folder index. It then converts \n",
    "# the images and labels lists to NumPy arrays, normalizes the pixel values to the range [0,1], and one-hot encodes the labels. \n",
    "# Finally, it splits the images and labels arrays into training and testing datasets using a 90/10 split ratio and \n",
    "# returns the resulting x_train, x_test, y_train, and y_test arrays.\n",
    "\n",
    "# edge_detection(self, image) is a helper method that takes an image as input and applies edge detection preprocessing on it. \n",
    "# The method converts the image to grayscale, applies Gaussian blur, adaptive thresholding, and Otsu thresholding to obtain a binary \n",
    "# image with edges detected, and returns the resulting image.\n",
    "\n",
    "class DataGatherer:\n",
    "\n",
    "  def __init__(self, *args):\n",
    "    if len(args) > 0:\n",
    "      self.dir = args[0]\n",
    "    elif len(args) == 0:\n",
    "      self.dir = \"\"\n",
    "\n",
    "\n",
    "  #this function loads the images along with their labels and apply\n",
    "  #pre-processing function on the images and finaly split them into train and\n",
    "  #test dataset\n",
    "  def load_images(self):\n",
    "    images = []\n",
    "    labels = []\n",
    "    index = -1\n",
    "    folders = sorted(os.listdir(self.dir))\n",
    "    \n",
    "    for folder in folders:\n",
    "      index += 1\n",
    "      \n",
    "      print(\"Loading images from folder \", folder ,\" has started.\")\n",
    "      for image in os.listdir(self.dir + '/' + folder):\n",
    "\n",
    "        img = cv2.imread(self.dir + '/' + folder + '/' + image,1)\n",
    "        \n",
    "        img = self.edge_detection(img)\n",
    "        img = cv2.resize(img, (64, 64))\n",
    "        img = img_to_array(img)\n",
    "\n",
    "        images.append(img)\n",
    "        labels.append(index)\n",
    "\n",
    "    images = np.array(images)\n",
    "    images = images.astype('float32')/255.0\n",
    "    labels = to_categorical(labels)\n",
    "\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.1)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "  def edge_detection(self, image):\n",
    "    minValue = 70\n",
    "    gray=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY) \n",
    "    blur = cv2.GaussianBlur(gray,(5,5),2)\n",
    "    th3 = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,11,2)\n",
    "    ret, res = cv2.threshold(th3, minValue, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac9f3d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from folder  A  has started.\n",
      "Loading images from folder  B  has started.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#loading the images from training directory\u001b[39;00m\n\u001b[0;32m      9\u001b[0m data_gatherer \u001b[38;5;241m=\u001b[39m DataGatherer(training_dir)\n\u001b[1;32m---> 12\u001b[0m x_train, x_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mdata_gatherer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mDataGatherer.load_images\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading images from folder \u001b[39m\u001b[38;5;124m\"\u001b[39m, folder ,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m has started.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m folder):\n\u001b[1;32m---> 42\u001b[0m   img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m   img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_detection(img)\n\u001b[0;32m     45\u001b[0m   img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(img, (\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# FOR PRACTICAL DONT RUN THIS FILE\n",
    "\n",
    "\n",
    "\n",
    "training_dir = '../dataset_3/asl_alphabet_train'\n",
    "test_dir='../dataset_3/asl_test_1/asl-alphabet-test'\n",
    "\n",
    "#loading the images from training directory\n",
    "data_gatherer = DataGatherer(training_dir)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = data_gatherer.load_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e1df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, training_size, and test_size are initialized with some values.\n",
    "# compute_steps_per_epoch is a lambda function that computes the number of steps per epoch based on the batch size and \n",
    "# the size of the training or test set.\n",
    "# steps_per_epoch and val_steps are computed using the compute_steps_per_epoch lambda function and the training and test set sizes.\n",
    "# An instance of the Model class is created, passing a Sequential object to the constructor.\n",
    "# Then, build_model() method of the Model class is called to create the model, and the resulting model is assigned to the classifier\n",
    "# variable.\n",
    "# The compile() method is called on the classifier object to configure the learning process with the optimizer, loss function,\n",
    "# and metrics.\n",
    "# The fit() method is called on the classifier object to train the model on the training data. The steps_per_epoch, epochs, \n",
    "# validation_data, and validation_steps parameters are passed to the fit() method.\n",
    "# The accuracy values during training and validation are stored in the history object returned by the fit() method.\n",
    "# The accuracy graph is plotted using the matplotlib library.\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "training_size = x_train.shape[0]\n",
    "test_size = x_test.shape[0]\n",
    "#computing steps and validation steps per epoch according to training\n",
    "#and testing size\n",
    "compute_steps_per_epoch = lambda x: int(ceil(1. * x/batch_size))\n",
    "steps_per_epoch = compute_steps_per_epoch(training_size)\n",
    "val_steps = compute_steps_per_epoch(test_size)\n",
    "\n",
    "\n",
    "#build the model\n",
    "classifier = Model(Sequential()).classifier\n",
    "classifier = Model.build_model(classifier)\n",
    "\n",
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#train the model\n",
    "history = classifier.fit(\n",
    "  x_train, y_train,\n",
    "  steps_per_epoch=steps_per_epoch,\n",
    "  epochs=10,\n",
    "  validation_data=(x_test, y_test),\n",
    "  validation_steps=val_steps)\n",
    "\n",
    "#plot accuracy graph\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='train_accuracy',)\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"classifier\")\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1545e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a Python class called GUI that provides methods to create and customize a graphical user interface using the Tkinter library.\n",
    "\n",
    "# __init__(self, title, size): The constructor method of the GUI class takes two arguments, a title string and a size string \n",
    "#     representing the dimensions of the window. It initializes the GUI by creating a new Tkinter object and \n",
    "#     setting the window's title and geometry.\n",
    "\n",
    "# create_frame(self, width, height, anchor, relx, rely, background='white'): This method creates a new frame widget within the GUI window. It takes six arguments, including the width and height of the frame in pixels, the anchor position, and the relative x and y coordinates of the frame within the window. It also accepts an optional background parameter to set the background color of the frame.\n",
    "\n",
    "# create_labels(self, label_num, labels, anchor, relx, rely, x_spacing=0, y_spacing=0, create_entrybox_per_label=False): \n",
    "#     This method creates one or more label widgets within the GUI window. It takes several arguments, \n",
    "#     including the number of labels to create, a list of label text strings, the anchor position, and the relative x and y coordinates \n",
    "#     of the first label. It also accepts optional parameters for x and y spacing between labels, and a flag to create a corresponding \n",
    "#     entry box widget for each label.\n",
    "\n",
    "# create_buttons(self, button_num, text, anchor, relx, rely, command=None, x_spacing=0, y_spacing=0): This method creates one or\n",
    "#     more button widgets within the GUI window. It takes several arguments, including the number of buttons to create, a list of button \n",
    "#     text strings, the anchor position, and the relative x and y coordinates of the first button. It also accepts optional parameters \n",
    "#     for a command to execute when the button is pressed, and x and y spacing between buttons.\n",
    "\n",
    "# Overall, this class provides a simple way to create and customize a GUI window with labels, buttons, and frames using the Tkinter\n",
    "# library.\n",
    "\n",
    "class GUI:\n",
    "    \n",
    "    def __init__(self, title, size):\n",
    "        self.root = Tk()\n",
    "        self.root.title(title)\n",
    "        self.root.geometry(size)\n",
    "\n",
    "    def create_frame(self, width, height, anchor, relx, rely, background='white'):\n",
    "        frame = Frame(self.root, bg=background, width=width, height=height)\n",
    "        frame.place(anchor=anchor, relx=relx, rely=rely)\n",
    "        return frame\n",
    "        \n",
    "    def create_labels(self, label_num, labels, anchor, relx, rely, x_spacing=0, y_spacing=0, create_entrybox_per_label=False):\n",
    "        entry_labels = {}\n",
    "        entry_boxes = {}\n",
    "        relx = relx\n",
    "        rely = rely\n",
    "\n",
    "        longest_label_spacing = len(max(labels, key=len))/100.0\n",
    "        \n",
    "        for i in range(label_num):\n",
    "            label = Label(self.root, text = labels[i]+\": \",\n",
    "                           font = (\"TimesNewRoman\", 15))\n",
    "            label.place(anchor=anchor, relx=relx, rely=rely)\n",
    "            \n",
    "            entry_labels[labels[i]] = label\n",
    "            if create_entrybox_per_label:\n",
    "                entry_box = Text(self.root, font=(\"TimesNewRoman\", 20), height=1, width=10)\n",
    "                entry_box.place(anchor=anchor, relx=relx+longest_label_spacing+0.02, rely=rely)\n",
    "                \n",
    "                entry_boxes[labels[i]+'_entrybox'] = entry_box\n",
    "            rely += y_spacing\n",
    "            relx += x_spacing\n",
    "        return entry_labels, entry_boxes\n",
    "\n",
    "    def create_buttons(self, button_num, text, anchor, relx, rely, command=None, x_spacing=0, y_spacing=0):\n",
    "        buttons = {}\n",
    "        relx = relx\n",
    "        rely = rely\n",
    "        \n",
    "        for i in range(button_num):\n",
    "            btn = Button(self.root, command=command, text=text[i])\n",
    "            btn.place(anchor=anchor, relx=relx, rely=rely)\n",
    "\n",
    "            buttons[text[i]+' button'] = btn\n",
    "            \n",
    "            rely += y_spacing\n",
    "            relx += x_spacing\n",
    "\n",
    "        return buttons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4052ebaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 205ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "character has been added:  t\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "character has been added:  y\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "character has been added:  t\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "character has been added:  f\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "character has been added:  y\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "character has been added:  a\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "# The code is an implementation of a sign language to text conversion application. \n",
    "# It uses a convolutional neural network to classify the hand gestures and then displays the text on the screen. \n",
    "# The user interface of the application is implemented using tkinter.\n",
    "\n",
    "# The first few lines import the required modules and load the pre-trained CNN model. The code then defines the draw_region() function,\n",
    "# which takes an image and a center point and returns a cropped version of the image centered around that point.\n",
    "\n",
    "# The start_gui() function creates a GUI window and returns a reference to it along with a label to display the video feed. The exit_app() function is used to close the GUI window and release the video stream when the application is closed.\n",
    "\n",
    "# The update_frame() function takes an image and a label and updates the label with the new image.\n",
    "\n",
    "# The get_threshold() function extracts a float value from a tkinter Entry widget.\n",
    "\n",
    "# The get_char() function takes a cropped image of a hand gesture, predicts the corresponding character using the CNN model, and returns the predicted character.\n",
    "\n",
    "# The spell_app() function converts the text entered in the textbox into speech using the Google Text-to-Speech API and saves it as an audio file.\n",
    "\n",
    "# The play() function plays the audio file.\n",
    "\n",
    "# The AddCharToWord() function takes a word and a character and updates the word accordingly. \n",
    "# If the character is a space, the Auto_Correct() function is called to autocorrect the word. \n",
    "# If the character is \"del,\" the last character of the word is deleted. If the character is not \"nothing,\" \n",
    "# the character is added to the word.\n",
    "\n",
    "# The frame_video_stream() function takes several arguments, including the names of the various tkinter widgets, \n",
    "# the current character, the previous character, the current word, and the current sentence. \n",
    "# It reads the video stream, crops the image around the center of the hand, and passes the cropped image to the CNN model \n",
    "# to predict the corresponding character. \n",
    "# It then updates the video feed with the predicted character and displays the current sentence on the screen.\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "cap = None\n",
    "\n",
    "# classifier =load_model('newCnn_git_v3.h5',compile=False)\n",
    "# classifier =load_model('newCnn_git_v4.h5')\n",
    "# classifier =load_model('newCnn_git_v6_noAug.h5')\n",
    "# classifier =load_model('newCnn_git_v7_Aug.h5')\n",
    "classifier =load_model('newCnn_git_v10_black_white.h5')\n",
    "\n",
    "\n",
    "def draw_region(image, center):\n",
    "    cropped_image = cv2.rectangle(image, (center[0] - 130, center[1] - 130),\n",
    "        (center[0] + 130, center[1] + 130), (0, 0, 255), 2)\n",
    "    return cropped_image[center[1]-130:center[1]+130, center[0]-130:center[0]+130], cropped_image\n",
    "\n",
    "def start_gui(title, size):\n",
    "    gui = GUI(title, size)\n",
    "\n",
    "    gui_frame = gui.create_frame(600, 600, 'ne', 1, 0, 'green')\n",
    "    vid_label = Label(gui_frame)\n",
    "    vid_label.grid()\n",
    "    \n",
    "    return gui, vid_label\n",
    "\n",
    "def exit_app(gui, cap):\n",
    "    gui.root.destroy()\n",
    "    cap.release()\n",
    "\n",
    "\n",
    "def update_frame(image, vid_label):\n",
    "    image_fromarray = Image.fromarray(image)\n",
    "    imgtk = ImageTk.PhotoImage(image=image_fromarray)\n",
    "    \n",
    "    vid_label.imgtk = imgtk\n",
    "    vid_label.config(image=imgtk)\n",
    "\n",
    "def get_threshold(label_entrybox):\n",
    "    value = label_entrybox.get('1.0', END)\n",
    "    try:\n",
    "        return float(value)\n",
    "    except:\n",
    "        return 0.95\n",
    "\n",
    "\n",
    "def get_char(gesture):\n",
    "    classes = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L',\n",
    "           'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V',\n",
    "           'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "\n",
    "    return Model.predict(classes, classifier, gesture)\n",
    "\n",
    "def spell_app(sentbox):\n",
    "    text=sentbox.get(\"1.0\",'end-1c')\n",
    "    language = 'en'\n",
    "    # Passing the text and language to the engine,  \n",
    "    # here we have marked slow=False. Which tells  \n",
    "    # the module that the converted audio should  \n",
    "    # have a high speed \n",
    "    myobj = gTTS(text=text, lang=language, slow=False) \n",
    "\n",
    "    # Saving the converted audio in a mp3 file named \n",
    "    path = Path('./Audio1.mp3')\n",
    "\n",
    "    print(path.is_file())\n",
    "    if(path.is_file()):\n",
    "        os.remove(\"Audio1.mp3\")\n",
    "        myobj.save(\"Audio1.mp3\")\n",
    "       \n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    myobj.save(\"Audio1.mp3\") \n",
    "def play():\n",
    "    playsound('Audio1.mp3')\n",
    "    \n",
    "\n",
    "\n",
    "def AddCharToWord(word, curr_char):\n",
    "    temp_word = word\n",
    "    if curr_char == 'space':\n",
    "        #print(Auto_Correct(temp_word))\n",
    "        temp_word = \"\"\n",
    "    elif curr_char == 'del':\n",
    "        temp_word = temp_word[0:-1]\n",
    "        print('character has been deleted')\n",
    "    elif curr_char != 'nothing':\n",
    "        temp_word += curr_char.lower()\n",
    "        print('character has been added: ', curr_char.lower())\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return [temp_word, curr_char]\n",
    "\n",
    "\n",
    "def frame_video_stream(names, curr_char, prev_char, word, sentence, *args):\n",
    "    kwargs = dict(zip(names, args))\n",
    "    \n",
    "    threshold = get_threshold(kwargs['th_box'])\n",
    "    curr_char = curr_char\n",
    "    prev_char = prev_char\n",
    "    \n",
    "    success, frame = cap.read()\n",
    "#     frame = cv2.flip(frame, 1)\n",
    "    # Flip the image horizontally for a later selfie-view display, and convert\n",
    "    # the BGR image to RGB.\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    update_frame(image, kwargs['vid_label'])\n",
    "\n",
    "    image.flags.writeable = False\n",
    "    results = kwargs['hands'].process(image)\n",
    "\n",
    "    # Draw the hand annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    image_height, image_width, _ = image.shape\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        \n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            x = [landmark.x for landmark in hand_landmarks.landmark]\n",
    "            y = [landmark.y for landmark in hand_landmarks.landmark]\n",
    "\n",
    "            \n",
    "            center = np.array([np.mean(x) * image_width, np.mean(y) * image_height]).astype('int32')\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            cropped_img, full_img = draw_region(image, center)\n",
    "\n",
    "            update_frame(full_img, kwargs['vid_label'])\n",
    "\n",
    "            try:\n",
    "                #print('from try')\n",
    "                # gray = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2GRAY)\n",
    "                gray = DataGatherer().edge_detection(cropped_img)\n",
    "\n",
    "                curr_char, pred = get_char(gray)\n",
    "                char = cv2.putText(full_img, curr_char, (center[0]-135, center[1]-135), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                char_prob = cv2.putText(full_img, '{0:.2f}'.format(np.max(pred)), (center[0]+60, center[1]-135), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "                update_frame(full_img, kwargs['vid_label'])\n",
    "\n",
    "                kwargs['cc_box'].delete('1.0', 'end')\n",
    "                kwargs['cc_box'].insert('end', curr_char)\n",
    "                #print(curr_char)\n",
    "                #compare the current char with the previous one and if matched, then won't add the current char\n",
    "                #because the model catches the chars realy quick and if the below if statement removed,\n",
    "                #the current char will be added endlessly to the word\n",
    "\n",
    "                #also we use the threshold to prevent the meaningless characters to be added to the word\n",
    "                #as the program catches the motion of the user's hand when the user changes the gesture(the motion between the gestures)\n",
    "                #and the program thinks\n",
    "                #it's a gesture and tries to match it with some letter but with low probability\n",
    "                if (curr_char != prev_char) and (np.max(pred)> threshold):\n",
    "                    #the below print statement is related to the formatter\n",
    "                    #print(pred)\n",
    "                    \n",
    "                    temp = AddCharToWord(word, curr_char)\n",
    "                    kwargs['ow_box'].insert('end', curr_char)\n",
    "                    \n",
    "                    if (temp[0] == \"\") and (temp[1] != \"del\"):\n",
    "                        sentence+= Auto_Correct(word) + \" \"\n",
    "                        kwargs['sent_box'].insert('end', Auto_Correct(word) + \" \")\n",
    "                        kwargs['ow_box'].delete('1.0', 'end')\n",
    "                        kwargs['cw_box'].delete('1.0', 'end')\n",
    "                        kwargs['cw_box'].insert('end', Auto_Correct(word))\n",
    "                    word = temp[0]\n",
    "\n",
    "                    prev_char = curr_char\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    kwargs['vid_label'].after(1, frame_video_stream, names, curr_char, prev_char, word, sentence, *args)\n",
    "    \n",
    "    \n",
    "    \n",
    "def pipe_cam(gui, vid_label):\n",
    "    \n",
    "    curr_char = None\n",
    "    prev_char = None\n",
    "    word = \"\"\n",
    "    sentence = \"\"\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #the predicted character won't be added to the word unless it's\n",
    "    #probability is higher than the threshold\n",
    "    #in places with good brightness and good camera the threshold can be a high value\n",
    "    #otherwise it should be a low value and the reason for that is in places that meet\n",
    "    #the above requirements, the model predict the letters with high probability to be\n",
    "    #the correct letter the user ment to add\n",
    "    threshold = float(0.95)\n",
    "\n",
    "    \n",
    "\n",
    "    #this formatter is to print the probability of the letters in readable\n",
    "    #format just for the programmers if they want to see what are the probabilities looking like\n",
    "    float_formatter = \"{:.5f}\".format\n",
    "    np.set_printoptions(formatter={'float_kind':float_formatter})\n",
    "    \n",
    "    global cap\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    labels_num = 5\n",
    "    labels = ['threshold', 'current char', 'original word', 'corrected word', 'sentence']\n",
    "\n",
    "    Labels, entryboxes = gui.create_labels(labels_num, labels, 'nw', 0, 0, y_spacing=0.06, create_entrybox_per_label=1)\n",
    "\n",
    "    entryboxes['original word_entrybox'].config(width=18)\n",
    "    entryboxes['corrected word_entrybox'].config(width=18)\n",
    "    entryboxes['sentence_entrybox'].config(width=18, height=8)\n",
    "    \n",
    "    \n",
    "    entryboxes['threshold_entrybox'].insert('end', threshold)\n",
    "    th_entrybox = entryboxes['threshold_entrybox']\n",
    "\n",
    "\n",
    "    cc_entrybox = entryboxes['current char_entrybox']\n",
    "\n",
    "\n",
    "    ow_entrybox = entryboxes['original word_entrybox']\n",
    "\n",
    "\n",
    "    cw_entrybox = entryboxes['corrected word_entrybox']\n",
    "\n",
    "\n",
    "    sent_entrybox = entryboxes['sentence_entrybox']\n",
    "    \n",
    "    \n",
    "    spell_program_btn = gui.create_buttons(1, ['CREATE SPEECH'], 'center', 0.2, 0.7, command=lambda: spell_app(entryboxes['sentence_entrybox']))\n",
    "    play_program_btn = gui.create_buttons(1, ['PLAY'], 'center', 0.4, 0.8, command=lambda: play())\n",
    "\n",
    "    \n",
    "    Exit_program_btn = gui.create_buttons(1, ['Exit'], 'center', 0.5, 0.9, command=lambda: exit_app(gui, cap))\n",
    "\n",
    "    names = ['vid_label', 'hands', 'th_box', 'cc_box', 'ow_box', 'cw_box', 'sent_box']\n",
    "    with mp_hands.Hands(\n",
    "            min_detection_confidence=0.4,\n",
    "            min_tracking_confidence=0.5,\n",
    "            max_num_hands=1) as hands:\n",
    "        \n",
    "            frame_video_stream(names, curr_char, prev_char, word, sentence, vid_label,\n",
    "                               hands,  th_entrybox, cc_entrybox, ow_entrybox, cw_entrybox, sent_entrybox)\n",
    "            gui.root.mainloop()\n",
    "\n",
    "\n",
    "title = \"Sign Language Recognition GUI\"\n",
    "size = \"1100x1100\"\n",
    "\n",
    "gui, vid_label = start_gui(title, size)\n",
    "\n",
    "pipe_cam(gui, vid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81d87bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
